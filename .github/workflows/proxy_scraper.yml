name: Proxy Scraper

on:
  workflow_dispatch: # Allows manual trigger
  schedule:
    - cron: '0 */6 * * *' # Runs every 6 hours (at 0:00, 6:00, 12:00, 18:00 UTC)

jobs:
  scrape-proxies:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9' # Recommended stable version. Consider 3.10 or 3.11 for newer features.

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Install all dependencies from requirements.txt first
        pip install -r requirements.txt
        # Then, explicitly upgrade httpx to ensure it's the latest version,
        # overriding any potential older version specified in requirements.txt.
        pip install --upgrade httpx

    - name: Ensure data and cache directories exist
      run: |
        mkdir -p data
        mkdir -p cache
        # GitHub Actions typically cleans the workspace, so these directories need to be created each run.

    - name: Run Proxy Scraper
      run: python proxy_scraper.py

    - name: Commit and push changes
      env:
        # Use the GitHub Token for authentication
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git add data/
        # Use --allow-empty if you want to create a commit even when no files have changed.
        # Otherwise, the '|| true' ensures the step doesn't fail if there are no changes.
        git commit -m "Update proxy nodes [skip ci]" || true
        git push
