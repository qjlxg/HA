import os
import urllib.parse
import base64
import logging
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('node_cleaning_errors.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def clean_duplicate_nodes_advanced(file_path, output_path=None):
    """
    è¯»å–æ–‡ä»¶ï¼ŒåŸºäºåè®®ç‰¹å®šè§£æé€»è¾‘ç§»é™¤é‡å¤è¡Œï¼Œä¿å­˜åˆ°æ–°æ–‡ä»¶ï¼Œå¹¶æä¾›è¯¦ç»†ç»Ÿè®¡æ•°æ®ã€‚
    æ”¯æŒ VLESSã€Trojanã€SS åè®®ï¼Œå¿½ç•¥éå…³é”®å­—æ®µï¼ˆå¦‚å¤‡æ³¨ã€fpï¼‰ï¼Œè®°å½•è§£æå¤±è´¥çš„èŠ‚ç‚¹ã€‚
    """
    if output_path is None:
        base, ext = os.path.splitext(file_path)
        output_path = f"{base}_cleaned{ext}"

    unique_node_keys = set()  # å­˜å‚¨å»é‡é”®
    unique_lines_output = []  # å­˜å‚¨åŸå§‹è¡Œ
    error_lines = []         # å­˜å‚¨è§£æå¤±è´¥çš„è¡Œ
    stats = defaultdict(int)  # æŒ‰åè®®ç»Ÿè®¡èŠ‚ç‚¹æ•°
    line_count = 0

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:  # æµå¼è¯»å–
                line_count += 1
                stripped_line = line.strip()
                if not stripped_line:
                    continue

                # åˆ†ç¦»æ ¸å¿ƒéƒ¨åˆ†å’Œå¤‡æ³¨
                hash_index = stripped_line.find('#')
                core_part = stripped_line[:hash_index].strip() if hash_index != -1 else stripped_line
                remark = stripped_line[hash_index:] if hash_index != -1 else ''

                # æå–åè®®å¹¶è®¡æ•°
                protocol = core_part.split('://')[0].lower()
                stats[protocol] += 1

                # è§£æå¹¶ç”Ÿæˆå»é‡é”®
                try:
                    node_key = generate_node_key(core_part)
                    if node_key and node_key not in unique_node_keys:
                        unique_node_keys.add(node_key)
                        unique_lines_output.append(line)
                    else:
                        stats[f"{protocol}_duplicates"] += 1
                except Exception as e:
                    logging.error(f"è§£æèŠ‚ç‚¹å¤±è´¥ (è¡Œ {line_count}): {stripped_line} | é”™è¯¯: {e}")
                    error_lines.append((line_count, stripped_line, str(e)))
                    stats[f"{protocol}_errors"] += 1
                    continue

        # å†™å…¥ç»“æœ
        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(unique_lines_output)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logging.info(f"âœ… æˆåŠŸæ¸…ç†é‡å¤èŠ‚ç‚¹ã€‚ç»“æœä¿å­˜åˆ°: {output_path}")
        logging.info(f"ğŸ“Š ç»Ÿè®¡æ•°æ®:")
        logging.info(f"  - åŸå§‹èŠ‚ç‚¹æ•°: {line_count}")
        logging.info(f"  - å”¯ä¸€èŠ‚ç‚¹æ•°: {len(unique_lines_output)}")
        logging.info(f"  - ç§»é™¤çš„é‡å¤èŠ‚ç‚¹æ•°: {line_count - len(unique_lines_output)}")
        logging.info(f"  - æŒ‰åè®®åˆ†ç±»:")
        for protocol in ['vless', 'trojan', 'ss']:
            logging.info(f"    - {protocol.upper()}: {stats[protocol]} èŠ‚ç‚¹, {stats[f'{protocol}_duplicates']} é‡å¤, {stats[f'{protocol}_errors']} è§£æå¤±è´¥")
        if error_lines:
            logging.warning(f"è§£æå¤±è´¥çš„èŠ‚ç‚¹æ•°: {len(error_lines)}ï¼Œè¯¦æƒ…è§ node_cleaning_errors.log")

        return True

    except FileNotFoundError:
        logging.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return False
    except Exception as e:
        logging.error(f"âŒ æ¸…ç†èŠ‚ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def generate_node_key(url):
    """æ ¹æ®åè®®ç”Ÿæˆå»é‡é”®ï¼Œä»…åŒ…å«å…³é”®å­—æ®µ"""
    parsed = urllib.parse.urlparse(url)
    scheme = parsed.scheme.lower()
    netloc = parsed.netloc
    query = parsed.query

    if scheme == "vless":
        return normalize_vless(url, netloc, query)
    elif scheme == "trojan":
        return normalize_trojan(url, netloc, query)
    elif scheme == "ss":
        return normalize_ss(url, netloc)
    else:
        # æœªè¯†åˆ«åè®®ï¼Œç›´æ¥è¿”å›åŸå§‹ URLï¼ˆå¯æ‰©å±•ä¸ºå…¶ä»–åè®®ï¼‰
        return url

def normalize_vless(url, netloc, query):
    """æ ‡å‡†åŒ– VLESS é“¾æ¥ï¼Œå¿½ç•¥éå…³é”®å­—æ®µ"""
    # æå– UUID å’Œ host:port
    uuid_host_port = netloc
    # è§£ææŸ¥è¯¢å‚æ•°ï¼Œä»…ä¿ç•™å…³é”®å­—æ®µ
    query_params = urllib.parse.parse_qs(query)
    key_params = {k: query_params[k] for k in ['type', 'path', 'security', 'encryption'] if k in query_params}
    sorted_query = urllib.parse.urlencode(key_params, doseq=True)
    return f"vless://{uuid_host_port}?{sorted_query}"

def normalize_trojan(url, netloc, query):
    """æ ‡å‡†åŒ– Trojan é“¾æ¥"""
    # Trojan çš„ netloc æ˜¯ password@host:port
    query_params = urllib.parse.parse_qs(query)
    key_params = {k: query_params[k] for k in ['type', 'sni'] if k in query_params}
    sorted_query = urllib.parse.urlencode(key_params, doseq=True)
    return f"trojan://{netloc}?{sorted_query}"

def normalize_ss(url, netloc):
    """æ ‡å‡†åŒ– SS é“¾æ¥"""
    try:
        if '@' in netloc:
            b64_config, host_port = netloc.split('@', 1)
            config = base64.urlsafe_b64decode(b64_config + '===').decode('utf-8')
            return f"ss://{config}@{host_port}"
        else:
            config = base64.urlsafe_b64decode(netloc + '===').decode('utf-8')
            return f"ss://{config}"
    except (base64.binascii.Error, UnicodeDecodeError) as e:
        raise ValueError(f"æ— æ³•è§£æ SS é…ç½®: {e}")

if __name__ == "__main__":
    nodes_file = os.path.join('data', 'a.isidomain.web.id.txt')
    clean_duplicate_nodes_advanced(nodes_file)
